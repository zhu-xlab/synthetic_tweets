{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(0) + \",\" + str(1) \n",
    "\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from itertools import islice\n",
    "\n",
    "print(os.getpid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import login\n",
    "\n",
    "# # Replace \"your_huggingface_token\" with your actual token\n",
    "# login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transformers\n",
    "# import tokenizers\n",
    "# print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model checkpoint and paths\n",
    "CHECKPOINT = \"unsloth/Llama-3.3-70B-Instruct-bnb-4bit\" # unsloth/Llama-3.3-70B-Instruct-bnb-4bit\n",
    "INPUT_FILE = \"./building_info_6000.jsonl\"\n",
    "OUTPUT_FILE = \"./generated_tweets.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT)\n",
    "\n",
    "# Define the quantization configuration\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,        # Specify Int8 quantization\n",
    "    # llm_int8_threshold=6.0    # Adjust this if needed for performance\n",
    ")\n",
    "\n",
    "# Load the model with sharding\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    CHECKPOINT,\n",
    "    device_map=\"balanced\",      # Automatically shard layers across GPUs/CPU\n",
    "    quantization_config=quantization_config,\n",
    "    torch_dtype=\"float16\",  # Use FP16 to save memory (optional)\n",
    "    # offload_folder=\"./offload\",  # Optional: Folder for CPU offloading if GPUs run out of memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create text generation pipeline\n",
    "text_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"balanced\",\n",
    "    repetition_penalty=1.4, # Discourage repetition\n",
    "    temperature=1.2,       # Increase randomness\n",
    "    top_k=60,              # Consider top 50 probable words\n",
    "    top_p=0.9,            # Use nucleus sampling\n",
    "    max_new_tokens=250,\n",
    "    return_full_text=False,  # To focus on the generated part only\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_prompt = (\n",
    "    \"Generate tweets as if they were posted by real Twitter users in a specific building.\\n\"\n",
    "    \"Tweets should sent from the type of building describes in 'building tag'.\\n\"\n",
    "    \"Diversity within tweets for a single building: Ensure that each tweet reflects a unique perspective or experience.\\n\"\n",
    "    \"Consider varying the tone (e.g., humorous, synic, formal, casual), the length (short and concise, longer and detailed), and the use of mention or hashtag.\\n\"\n",
    "    \"Diversity across buildings: Avoid reusing templates or expressions between buildings.\\n\"\n",
    "    \"Imaginative Scenarios: Highlight varied aspects of the building, such as its architecture, services, location, history, or events. Be creative and explore different angles.\\n\"\n",
    "    \"Personas: Imagine switching personas for each tweet, simulating thoughts from different types of users, such as tourists, professionals, or families.\\n\"\n",
    "    \"You must generate only one tweet in each language specified under 'tweet language distribution', written directly in that language.\\n\\n\"\n",
    "    \"Returns output in this format: {building id: list of generated tweets}\"\n",
    "    \n",
    "    \"Example:\\n\\n\"\n",
    "    '{\"building id\":227579, \"building city\": \"London\", \"building tags\": \"apartments\", \"building names\": \"Moo\", \"tweet language distribution\": [\"English\", \"German\", \"Chinese\"]}\\n\\n'\n",
    "    '{227579: [\"Finally moved in my little aprtment in London! #NewBeginnings\", \"@Viola Erstaunlich ruhig, trotz der zentralen Lage.\", \"最近在练习冥想，好像时间都慢下来了。\"]}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format data into a prompt\n",
    "def format_data_prompt(metadata):\n",
    "    return json.dumps(metadata, ensure_ascii=False) + ' returns {building id: list of generated tweets}'\n",
    "\n",
    "# Process a single line of metadata\n",
    "def process_metadata(metadata):\n",
    "    data_prompt = format_data_prompt(metadata)\n",
    "    prompt = [\n",
    "        {\"role\": \"system\", \"content\": context_prompt},\n",
    "        {\"role\": \"user\", \"content\": data_prompt}]\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to write results iteratively\n",
    "def write_result(outfile, index, result):\n",
    "    try:\n",
    "        outfile.write(json.dumps({\"index\": index, \"output\": result}, ensure_ascii=False) + \"\\n\")\n",
    "        outfile.flush()  # Immediately save to disk\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing result for index {index}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_ROW = 1\n",
    "\n",
    "with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as infile, open(OUTPUT_FILE, \"a\", encoding=\"utf-8\") as outfile:\n",
    "    # Skip the first START_ROW - 1 rows\n",
    "    for index, line in enumerate(tqdm(islice(infile, START_ROW - 1, None), desc=\"Processing buildings\"), start=START_ROW):\n",
    "        try:\n",
    "            metadata = json.loads(line)\n",
    "            prompt = process_metadata(metadata)\n",
    "\n",
    "            # Call the text generation pipeline\n",
    "            result = text_pipeline(prompt)\n",
    "            generated_text = result[0][\"generated_text\"]\n",
    "\n",
    "            # Write the generated result with the index\n",
    "            write_result(outfile, index, generated_text)\n",
    "        except Exception as e:\n",
    "            # Write \"failed\" with the exception message\n",
    "            write_result(outfile, index, str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define your index list\n",
    "# INDEX_LIST = {4354,5946}  # Replace with the actual indices you need\n",
    "\n",
    "# # Open input and output files\n",
    "# with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as infile, open(OUTPUT_FILE, \"a\", encoding=\"utf-8\") as outfile:\n",
    "#     for index, line in enumerate(tqdm(infile, desc=\"Processing buildings\"), start=1):\n",
    "#         if index in INDEX_LIST:  # Process only if index is in the list\n",
    "#             try:\n",
    "#                 metadata = json.loads(line)\n",
    "#                 prompt = process_metadata(metadata)\n",
    "\n",
    "#                 # Call the text generation pipeline\n",
    "#                 result = text_pipeline(prompt)\n",
    "#                 generated_text = result[0][\"generated_text\"]\n",
    "\n",
    "#                 # Write the generated result with the index\n",
    "#                 write_result(outfile, index, generated_text)\n",
    "#             except Exception as e:\n",
    "#                 # Write \"failed\" with the exception message\n",
    "#                 write_result(outfile, index, str(e))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
