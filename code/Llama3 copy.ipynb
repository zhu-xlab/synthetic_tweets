{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process ID: 1855\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(1) + \",\" + str(2) \n",
    "\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from itertools import islice\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "\n",
    "print(f\"Process ID: {os.getpid()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Model checkpoint and paths\n",
    "# CHECKPOINT = \"unsloth/Llama-3.3-70B-Instruct-bnb-4bit\"\n",
    "# CACHE_DIR = \"/mntssd/mnt3/shanshanbai/my_storage_from_qian/.cache/huggingface/hub/\"\n",
    "# INPUT_FILE = \"/mntssd/mnt3/shanshanbai/my_storage_from_qian/results/commercial_buildings_7980.jsonl\"\n",
    "# OUTPUT_FILE = \"/mntssd/mnt3/shanshanbai/my_storage_from_qian/results/generated_tweets__3500.jsonl\"\n",
    "\n",
    "# Model checkpoint and paths\n",
    "CHECKPOINT = \"unsloth/Llama-3.3-70B-Instruct-bnb-4bit\" # unsloth/Llama-3.3-70B-Instruct-bnb-4bit\n",
    "CACHE_DIR = \"/mntssd/mnt3/shanshanbai/my_storage_from_qian/.cache/huggingface/hub/\"\n",
    "INPUT_FILE = \"/mntssd/mnt3/shanshanbai/my_storage_from_qian/results/residential_buildings_6558.jsonl\"\n",
    "OUTPUT_FILE = \"/mntssd/mnt3/shanshanbai/my_storage_from_qian/results/generated_tweets_residential_6000.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "/usr/local/lib/python3.10/site-packages/transformers/quantizers/auto.py:186: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b849b5df3a44be6b02c65ffcb716453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "facb4003c6c64bad8ae595e42a08b51e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT)\n",
    "\n",
    "# Define the quantization configuration\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,        # Specify Int8 quantization\n",
    "    # llm_int8_threshold=6.0    # Adjust this if needed for performance\n",
    ")\n",
    "\n",
    "# Load the model with sharding\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    CHECKPOINT,\n",
    "    device_map=\"balanced\",      # Automatically shard layers across GPUs/CPU\n",
    "    quantization_config=quantization_config,\n",
    "    torch_dtype=\"float16\",  # Use FP16 to save memory (optional)\n",
    "    # offload_folder=\"./offload\",  # Optional: Folder for CPU offloading if GPUs run out of memory\n",
    "    cache_dir=CACHE_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Create text generation pipeline\n",
    "text_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"balanced\",\n",
    "    repetition_penalty=1.3, # Discourage repetition\n",
    "    temperature=1.4,       # Increase randomness\n",
    "    top_k=100,              # Consider top 50 probable words\n",
    "    top_p=0.9,            # Use nucleus sampling\n",
    "    max_new_tokens=250,\n",
    "    return_full_text=False,  # To focus on the generated part only\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context_prompt = (\n",
    "#     \"Generate tweets as if they were posted by real Twitter users in a specific commercial building.\\n\"\n",
    "#     \"Diversity within tweets for a single commercial building: Ensure that each tweet reflects a unique perspective or experience.\\n\"\n",
    "#     \"Consider varying the tone (e.g., humorous, synic, formal, casual), the length (short and concise, longer and detailed), and the use of emojis or hashtags.\\n\"\n",
    "#     \"Diversity across commercial buildings: Avoid reusing templates or expressions between buildings.\\n\"\n",
    "#     \"Imaginative Scenarios: Highlight varied aspects of the building, such as its architecture, services, location, history, or events. Be creative and explore different angles.\\n\"\n",
    "#     \"Personas: Imagine switching personas for each tweet, simulating thoughts from different types of users, such as tourists, professionals, or families.\\n\"\n",
    "#     \"Incorporate building-specific details like tags and names naturally, without overemphasis or repetition.\\n\"\n",
    "#     \"Tweet should not start with 'Just', 'Scored','Love','Loving','Shopped','Shopping','Spend','Shout'\\n\"\n",
    "#     \"You must generate only one tweet in each language specified under 'tweet language distribution', written directly in that language.\\n\\n\"\n",
    "    \n",
    "#     \"Example:\\n\\n\"\n",
    "#     '{\"building id\":227579, \"building tags\": \"university\", \"building names\": [\"Institute of Psychiatry, Psychology & Neuroscience\"], \"tweet language distribution\": [\"English\", \"Chinese\", \"Chinese\"]}\\n\\n'\n",
    "#     '{227579: [\"Groundbreaking work they\\'re doing in mental health research @IPPN #ResearchLife.\", \"了解到了许多关于心理健康的研究 👏👏\", \"你的母校真豪啊！ @Mengshan\"]}'\n",
    "# )\n",
    "\n",
    "context_prompt = (\n",
    "    \"Generate tweets as if they were posted by real Twitter users in a residential building.\\n\"\n",
    "    \"Diversity within tweets for a single residential building: Ensure that each tweet reflects a unique perspective or experience.\\n\"\n",
    "    \"Consider varying the tone (e.g., humorous, synic, formal, casual), the length (short and concise, longer and detailed), and the use of mention, emojis or hashtags.\\n\"\n",
    "    \"Imaginative Scenarios: Highlight varied aspects of the building, such as its services, location, or activities, etc. Be creative and explore more angles.\\n\"\n",
    "    \"Personas: Imagine switching personas for each tweet, simulating thoughts from different types of users, such as teenagers, friends, relatives or families.\\n\"\n",
    "    \"Incorporate building-specific details like building tags implicitly without overemphasize, and do not mention the building name. \\n\"\n",
    "    \"Tweet should not start with 'Just','Love','Loving','Exciting','Excited','Woke','Still','Feeling','Moved','Die Aussicht','Je','My','Really' in any language.\"\n",
    "    \"You shoule only generate one tweet in each language specified under 'tweet language distribution' list provided, and written directly in that language.\\n\\n\"\n",
    "    \n",
    "    \"Example:\\n\\n\"\n",
    "    '{\"building id\":227579, building city: \"London\", \"building tags\": \"apartments\", \"building names\": \"Moo\", \"tweet language distribution\": [\"English\", \"German\", \"Chinese\"]}\\n\\n'\n",
    "    '{227579: [\"Finally moved in my little aprtment in London! 🏡✨ #NewBeginnings\", \"@Viola Erstaunlich ruhig, trotz der zentralen Lage \", \"偶尔冥想，好像时间都慢下来了 🧘\"]}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to write results iteratively\n",
    "def write_result(outfile, index, result):\n",
    "    try:\n",
    "        outfile.write(json.dumps({\"index\": index, \"output\": result}, ensure_ascii=False) + \"\\n\")\n",
    "        outfile.flush()  # Immediately save to disk\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing result for index {index}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format data into a prompt\n",
    "def format_data_prompt(metadata):\n",
    "    return json.dumps(metadata, ensure_ascii=False) + ' returns {building id: list of generated tweets}'\n",
    "\n",
    "# Process a single line of metadata\n",
    "def process_metadata(metadata):\n",
    "    data_prompt = format_data_prompt(metadata)\n",
    "    prompt = [\n",
    "        {\"role\": \"system\", \"content\": context_prompt},\n",
    "        {\"role\": \"user\", \"content\": data_prompt}]\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing buildings: 2it [04:06, 114.62s/it]"
     ]
    }
   ],
   "source": [
    "# Start processing from the 4000th row\n",
    "START_ROW = 6409\n",
    "\n",
    "with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as infile, open(OUTPUT_FILE, \"a\", encoding=\"utf-8\") as outfile:\n",
    "    # Skip the first START_ROW - 1 rows\n",
    "    for index, line in enumerate(tqdm(islice(infile, START_ROW - 1, None), desc=\"Processing buildings\"), start=START_ROW):\n",
    "        try:\n",
    "            metadata = json.loads(line)\n",
    "            prompt = process_metadata(metadata)\n",
    "\n",
    "            # Call the text generation pipeline\n",
    "            result = text_pipeline(prompt)\n",
    "            generated_text = result[0][\"generated_text\"]\n",
    "\n",
    "            # Write the generated result with the index\n",
    "            write_result(outfile, index, generated_text)\n",
    "        except Exception as e:\n",
    "            # Write \"failed\" with the exception message\n",
    "            write_result(outfile, index, str(e))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
