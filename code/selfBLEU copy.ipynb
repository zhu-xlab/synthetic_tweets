{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from sacrebleu.metrics import BLEU\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-multilingual-cased\", cache_dir=\"/mntssd/mnt3/shanshanbai/my_storage_from_qian/.cache/huggingface/hub\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('/mntssd/mnt3/shanshanbai/nlpinearthobservation/synthetic_data/data/synthetic.csv')\n",
    "# df = pd.read_csv('/mntssd/mnt3/shanshanbai/my_storage_from_qian/results/generated tweets/real.csv')\n",
    "\n",
    "# # Undersample the dataset for balanced class distribution\n",
    "# min_class_size = 3000\n",
    "# df = (\n",
    "#     df.groupby('mapped_class')\n",
    "#     .apply(lambda x: x.sample(n=min_class_size, random_state=42))\n",
    "#     .reset_index(drop=True)\n",
    "# )\n",
    "\n",
    "# print(\"Class distribution after undersampling:\\n\", df['mapped_class'].value_counts())\n",
    "\n",
    "# Process tweets\n",
    "df['tweets'] = df['tweets'].apply(ast.literal_eval) # tweet_no_url\n",
    "all_tweets = [tweet for sublist in df['tweets'] for tweet in sublist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16098"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_tweets = [\" \".join(tokenizer.tokenize(tweet)) for tweet in all_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BLEU scorer\n",
    "bleu = BLEU(effective_order=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute self-BLEU\n",
    "def compute_self_bleu(tokenized_texts, n_gram=4):\n",
    "    scores = []\n",
    "    \n",
    "    for i, ref in enumerate(tokenized_texts):\n",
    "        references = [t for j, t in enumerate(tokenized_texts) if i != j]  # Exclude current tweet\n",
    "        score = bleu.corpus_score([ref], [references]).score  # Compute BLEU score\n",
    "        scores.append(score)\n",
    "\n",
    "    return np.mean(scores)  # Average self-BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Compute 4-gram self-BLEU\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m self_bleu_score \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_self_bleu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenized_tweets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_gram\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4-gram Self-BLEU Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mself_bleu_score\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[15], line 7\u001b[0m, in \u001b[0;36mcompute_self_bleu\u001b[0;34m(tokenized_texts, n_gram)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, ref \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tokenized_texts):\n\u001b[1;32m      6\u001b[0m     references \u001b[38;5;241m=\u001b[39m [t \u001b[38;5;28;01mfor\u001b[39;00m j, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tokenized_texts) \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m j]  \u001b[38;5;66;03m# Exclude current tweet\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[43mbleu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorpus_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mref\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mreferences\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mscore  \u001b[38;5;66;03m# Compute BLEU score\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     scores\u001b[38;5;241m.\u001b[39mappend(score)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(scores)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/sacrebleu/metrics/base.py:437\u001b[0m, in \u001b[0;36mMetric.corpus_score\u001b[0;34m(self, hypotheses, references, n_bootstrap)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_corpus_score_args(hypotheses, references)\n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m# Collect corpus stats\u001b[39;00m\n\u001b[0;32m--> 437\u001b[0m stats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_corpus_statistics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhypotheses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreferences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;66;03m# Compute the actual system score\u001b[39;00m\n\u001b[1;32m    440\u001b[0m actual_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aggregate_and_compute(stats)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/sacrebleu/metrics/base.py:374\u001b[0m, in \u001b[0;36mMetric._extract_corpus_statistics\u001b[0;34m(self, hypotheses, references)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;66;03m# Pre-compute references\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;66;03m# Don't store the cache as the user is explicitly passing refs\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m references:\n\u001b[0;32m--> 374\u001b[0m     ref_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache_references\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreferences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ref_cache:\n\u001b[1;32m    376\u001b[0m     ref_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ref_cache\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/sacrebleu/metrics/base.py:349\u001b[0m, in \u001b[0;36mMetric._cache_references\u001b[0;34m(self, references)\u001b[0m\n\u001b[1;32m    346\u001b[0m     lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess_segment(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m lines]\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;66;03m# Get n-grams\u001b[39;00m\n\u001b[0;32m--> 349\u001b[0m     ref_cache\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_reference_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlines\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(num_refs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(num_refs)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/sacrebleu/metrics/bleu.py:358\u001b[0m, in \u001b[0;36mBLEU._extract_reference_info\u001b[0;34m(self, refs)\u001b[0m\n\u001b[1;32m    354\u001b[0m ref_lens \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ref \u001b[38;5;129;01min\u001b[39;00m refs:\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;66;03m# extract n-grams for this ref\u001b[39;00m\n\u001b[0;32m--> 358\u001b[0m     this_ngrams, ref_len \u001b[38;5;241m=\u001b[39m \u001b[43mextract_all_word_ngrams\u001b[49m\u001b[43m(\u001b[49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_ngram_order\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    359\u001b[0m     ref_lens\u001b[38;5;241m.\u001b[39mappend(ref_len)\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ngrams \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    362\u001b[0m         \u001b[38;5;66;03m# Set it directly for first set of refs\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/sacrebleu/metrics/helpers.py:23\u001b[0m, in \u001b[0;36mextract_all_word_ngrams\u001b[0;34m(line, min_order, max_order)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(tokens) \u001b[38;5;241m-\u001b[39m n \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     21\u001b[0m         ngrams\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mtuple\u001b[39m(tokens[i: i \u001b[38;5;241m+\u001b[39m n]))\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCounter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mngrams\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mlen\u001b[39m(tokens)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/collections/__init__.py:577\u001b[0m, in \u001b[0;36mCounter.__init__\u001b[0;34m(self, iterable, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''Create a new, empty Counter object.  And if given, count elements\u001b[39;00m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;124;03mfrom an input iterable.  Or, initialize the count from another mapping\u001b[39;00m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;124;03mof elements to their counts.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    574\u001b[0m \n\u001b[1;32m    575\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m--> 577\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/collections/__init__.py:670\u001b[0m, in \u001b[0;36mCounter.update\u001b[0;34m(self, iterable, **kwds)\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mupdate(iterable)\n\u001b[1;32m    669\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 670\u001b[0m         \u001b[43m_count_elements\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds:\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(kwds)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Compute 4-gram self-BLEU\n",
    "self_bleu_score = compute_self_bleu(tokenized_tweets, n_gram=4)\n",
    "\n",
    "print(f\"4-gram Self-BLEU Score: {self_bleu_score:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
