{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(0) + \",\" + str(1) \n",
    "\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1262\n"
     ]
    }
   ],
   "source": [
    "print(os.getpid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import login\n",
    "\n",
    "# # Replace \"your_huggingface_token\" with your actual token\n",
    "# login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transformers\n",
    "# import tokenizers\n",
    "# print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model checkpoint and paths\n",
    "CHECKPOINT = \"unsloth/Llama-3.3-70B-Instruct-bnb-4bit\" # unsloth/Llama-3.3-70B-Instruct-bnb-4bit\n",
    "CACHE_DIR = \"/mntssd/mnt3/shanshanbai/my_storage_from_qian/.cache/huggingface/hub/\"\n",
    "INPUT_FILE = \"/mntssd/mnt3/shanshanbai/my_storage_from_qian/results/generated tweets/building_info_6000.jsonl\"\n",
    "OUTPUT_FILE = \"/mntssd/mnt3/shanshanbai/my_storage_from_qian/results/generated tweets/tweets_better_6000.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "/usr/local/lib/python3.10/site-packages/transformers/quantizers/auto.py:195: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dabc10ed50644c46bd32669a433fb89a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT)\n",
    "\n",
    "# Define the quantization configuration\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,        # Specify Int8 quantization\n",
    "    # llm_int8_threshold=6.0    # Adjust this if needed for performance\n",
    ")\n",
    "\n",
    "# Load the model with sharding\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    CHECKPOINT,\n",
    "    device_map=\"balanced\",      # Automatically shard layers across GPUs/CPU\n",
    "    quantization_config=quantization_config,\n",
    "    torch_dtype=\"float16\",  # Use FP16 to save memory (optional)\n",
    "    # offload_folder=\"./offload\",  # Optional: Folder for CPU offloading if GPUs run out of memory\n",
    "    cache_dir=CACHE_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Create text generation pipeline\n",
    "text_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"balanced\",\n",
    "    repetition_penalty=1.4, # Discourage repetition\n",
    "    temperature=1.2,       # Increase randomness\n",
    "    top_k=60,              # Consider top 50 probable words\n",
    "    top_p=0.9,            # Use nucleus sampling\n",
    "    max_new_tokens=250,\n",
    "    return_full_text=False,  # To focus on the generated part only\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_prompt = (\n",
    "    \"Generate tweets as if they were posted by real Twitter users in a specific building.\\n\"\n",
    "    \"Tweets should sent from the type of building describes in 'building tag'.\\n\"\n",
    "    \"Diversity within tweets for a single building: Ensure that each tweet reflects a unique perspective or experience.\\n\"\n",
    "    \"Consider varying the tone (e.g., humorous, synic, formal, casual), the length (short and concise, longer and detailed), and the use of mention or hashtag.\\n\"\n",
    "    \"Diversity across buildings: Avoid reusing templates or expressions between buildings.\\n\"\n",
    "    \"Imaginative Scenarios: Highlight varied aspects of the building, such as its architecture, services, location, history, or events. Be creative and explore different angles.\\n\"\n",
    "    \"Personas: Imagine switching personas for each tweet, simulating thoughts from different types of users, such as tourists, professionals, or families.\\n\"\n",
    "    \"Do not mention building names if it's a residential building.\"\n",
    "    \"Tweet should not start with 'Just', 'Scored','Love','Loving','Shopped','Living','Shopping','Spend','Shout','Exploring','Ran', 'Spent','Finally','Exciting','Excited','Woke','Still','Feeling','Moved','Die Aussicht','Je','My','Really'\\n\"\n",
    "    \"You must generate only one tweet in each language specified under 'tweet language distribution', written directly in that language.\\n\\n\"\n",
    "    \"Returns output in this format: {building id: list of generated tweets}\"\n",
    "    \n",
    "    \"Example:\\n\\n\"\n",
    "    '{\"building id\":227579, \"building city\": \"London\", \"building tags\": \"apartments\", \"building names\": \"Moo\", \"tweet language distribution\": [\"English\", \"German\", \"Chinese\"]}\\n\\n'\n",
    "    '{227579: [\"Finally moved in my little aprtment in London! #NewBeginnings\", \"@Viola Erstaunlich ruhig, trotz der zentralen Lage.\", \"最近在练习冥想，好像时间都慢下来了。\"]}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format data into a prompt\n",
    "def format_data_prompt(metadata):\n",
    "    return json.dumps(metadata, ensure_ascii=False) + ' returns {building id: list of generated tweets}'\n",
    "\n",
    "# Process a single line of metadata\n",
    "def process_metadata(metadata):\n",
    "    data_prompt = format_data_prompt(metadata)\n",
    "    prompt = [\n",
    "        {\"role\": \"system\", \"content\": context_prompt},\n",
    "        {\"role\": \"user\", \"content\": data_prompt}]\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to write results iteratively\n",
    "def write_result(outfile, index, result):\n",
    "    try:\n",
    "        outfile.write(json.dumps({\"index\": index, \"output\": result}, ensure_ascii=False) + \"\\n\")\n",
    "        outfile.flush()  # Immediately save to disk\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing result for index {index}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Start processing from the 4000th row\n",
    "# START_ROW = 1917\n",
    "\n",
    "# with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as infile, open(OUTPUT_FILE, \"a\", encoding=\"utf-8\") as outfile:\n",
    "#     # Skip the first START_ROW - 1 rows\n",
    "#     for index, line in enumerate(tqdm(islice(infile, START_ROW - 1, None), desc=\"Processing buildings\"), start=START_ROW):\n",
    "#         try:\n",
    "#             metadata = json.loads(line)\n",
    "#             prompt = process_metadata(metadata)\n",
    "\n",
    "#             # Call the text generation pipeline\n",
    "#             result = text_pipeline(prompt)\n",
    "#             generated_text = result[0][\"generated_text\"]\n",
    "\n",
    "#             # Write the generated result with the index\n",
    "#             write_result(outfile, index, generated_text)\n",
    "#         except Exception as e:\n",
    "#             # Write \"failed\" with the exception message\n",
    "#             write_result(outfile, index, str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing buildings: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing buildings: 6000it [04:44, 21.08it/s]\n"
     ]
    }
   ],
   "source": [
    "# Define your index list\n",
    "INDEX_LIST = {4354,5946}  # Replace with the actual indices you need\n",
    "\n",
    "# Open input and output files\n",
    "with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as infile, open(OUTPUT_FILE, \"a\", encoding=\"utf-8\") as outfile:\n",
    "    for index, line in enumerate(tqdm(infile, desc=\"Processing buildings\"), start=1):\n",
    "        if index in INDEX_LIST:  # Process only if index is in the list\n",
    "            try:\n",
    "                metadata = json.loads(line)\n",
    "                prompt = process_metadata(metadata)\n",
    "\n",
    "                # Call the text generation pipeline\n",
    "                result = text_pipeline(prompt)\n",
    "                generated_text = result[0][\"generated_text\"]\n",
    "\n",
    "                # Write the generated result with the index\n",
    "                write_result(outfile, index, generated_text)\n",
    "            except Exception as e:\n",
    "                # Write \"failed\" with the exception message\n",
    "                write_result(outfile, index, str(e))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
