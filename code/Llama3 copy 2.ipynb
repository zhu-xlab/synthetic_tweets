{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process ID: 1302\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(4) + \",\" + str(5) \n",
    "\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from itertools import islice\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "print(f\"Process ID: {os.getpid()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Model checkpoint and paths\n",
    "# CHECKPOINT = \"unsloth/Llama-3.3-70B-Instruct-bnb-4bit\"\n",
    "# CACHE_DIR = \"/mntssd/mnt3/shanshanbai/my_storage_from_qian/.cache/huggingface/hub/\"\n",
    "# INPUT_FILE = \"/mntssd/mnt3/shanshanbai/my_storage_from_qian/results/commercial_buildings_7980.jsonl\"\n",
    "# OUTPUT_FILE = \"/mntssd/mnt3/shanshanbai/my_storage_from_qian/results/generated_tweets__.jsonl\"\n",
    "\n",
    "# Model checkpoint and paths\n",
    "CHECKPOINT = \"unsloth/Llama-3.3-70B-Instruct-bnb-4bit\" # unsloth/Llama-3.3-70B-Instruct-bnb-4bit\n",
    "CACHE_DIR = \"/mntssd/mnt3/shanshanbai/my_storage_from_qian/.cache/huggingface/hub/\"\n",
    "INPUT_FILE = \"/mntssd/mnt3/shanshanbai/my_storage_from_qian/results/residential_buildings_6558.jsonl\"\n",
    "OUTPUT_FILE = \"/mntssd/mnt3/shanshanbai/my_storage_from_qian/results/generated_tweets_residential_4000.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "/usr/local/lib/python3.10/site-packages/transformers/quantizers/auto.py:186: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6644e21994be4c12969a3431a926f0b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT)\n",
    "\n",
    "# Define the quantization configuration\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,        # Specify Int8 quantization\n",
    "    # llm_int8_threshold=6.0    # Adjust this if needed for performance\n",
    ")\n",
    "\n",
    "# Load the model with sharding\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    CHECKPOINT,\n",
    "    device_map=\"balanced\",      # Automatically shard layers across GPUs/CPU\n",
    "    quantization_config=quantization_config,\n",
    "    torch_dtype=\"float16\",  # Use FP16 to save memory (optional)\n",
    "    # offload_folder=\"./offload\",  # Optional: Folder for CPU offloading if GPUs run out of memory\n",
    "    cache_dir=CACHE_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Create text generation pipeline\n",
    "text_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"balanced\",\n",
    "    repetition_penalty=1.3, # Discourage repetition\n",
    "    temperature=1.4,       # Increase randomness\n",
    "    top_k=100,              # Consider top 50 probable words\n",
    "    top_p=0.9,            # Use nucleus sampling\n",
    "    max_new_tokens=250,\n",
    "    return_full_text=False,  # To focus on the generated part only\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context_prompt = (\n",
    "#     \"You are tasked with generating tweets as if they were posted by real Twitter users in a specific commercial building.\\n\"\n",
    "#     \"Diversity within tweets for a single commercial building: Each tweet should reflect a unique experience or perspective.\\n\"\n",
    "#     \"Diversity across commercial buildings: Tweets for different commercial buildings should not reuse the same templates or expressions from previous buildings.\\n\"\n",
    "#     \"Minimized Template Overlap: Use a larger pool of unique expressions and ensure non-repetition across tweets and buildings.\\n\"\n",
    "#     \"User Simulation: Imagine switching personas for every new tweet to simulate multiple Twitter users. Treat each tweet as a unique thought or experience from a different user. You can also use @, # and emojis sometimes.\\n\"\n",
    "#     \"Imaginative Scenarios: Generate tweets that highlight varied aspects such as architecture, service, food, events, or location.\\n\"\n",
    "#     \"Use all building tags and all building names of a building as inspiration. Incorporate them naturally without overemphasis or repetition.\\n\"\n",
    "#     \"Tweet should not start with 'Just', 'Scored','Love','Loving','Shopped','Shopping','Spend','Shout','Exploring'\\n\"\n",
    "#     \"You must generate only one tweet in each language specified under 'tweet language distribution', written directly in that language.\\n\\n\"\n",
    "    \n",
    "#     \"Example:\\n\\n\"\n",
    "#     '{\"building id\":227579, \"building tags\": \"university\", \"building names\": [\"Institute of Psychiatry, Psychology & Neuroscience\"], \"tweet language distribution\": [\"English\", \"Chinese\", \"Chinese\"]}\\n\\n'\n",
    "#     '{227579:[\"Truly inspiring to see the groundbreaking work they\\'re doing in mental health research @IPPN # ResearchLife .\", \"了解到了许多关于心理健康的研究 👏👏\", \"有钱真好，羡慕这里的学生。@Mengshan\"]}'\n",
    "# )\n",
    "\n",
    "context_prompt = (\n",
    "    \"Generate tweets as if they were posted by real Twitter users in a residential building.\\n\"\n",
    "    \"Diversity within tweets for a single residential building: Ensure that each tweet reflects a unique perspective or experience.\\n\"\n",
    "    \"Consider varying the tone (e.g., humorous, synic, formal, casual), the length (short and concise, longer and detailed), and the use of mention, emojis or hashtags.\\n\"\n",
    "    \"Imaginative Scenarios: Highlight varied aspects of the building, such as its services, location, or activities, etc. Be creative and explore more angles.\\n\"\n",
    "    \"Personas: Imagine switching personas for each tweet, simulating thoughts from different types of users, such as teenagers, friends, relatives or families.\\n\"\n",
    "    \"Incorporate building-specific details like building tags implicitly without overemphasize, and do not mention the building name. \\n\"\n",
    "    \"Tweet should not start with 'Just','Love','Loving','Exciting','Excited','Woke','Still','Feeling','My','Moved','Die Aussicht','Je' in any language.\"\n",
    "    \"You shoule only generate one tweet in each language specified under 'tweet language distribution' list provided, and written directly in that language.\\n\\n\"\n",
    "    \n",
    "    \"Example:\\n\\n\"\n",
    "    '{\"building id\":227579, building city: \"London\", \"building tags\": \"apartments\", \"building names\": \"Moo\", \"tweet language distribution\": [\"English\", \"German\", \"Chinese\"]}\\n\\n'\n",
    "    '{227579: [\"Finally moved in my little aprtment in London! 🏡✨ #NewBeginnings\", \"@Viola Erstaunlich ruhig, trotz der zentralen Lage \", \"偶尔冥想，好像时间都慢下来了 🧘\"]}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format data into a prompt\n",
    "def format_data_prompt(metadata):\n",
    "    return json.dumps(metadata, ensure_ascii=False) + ' returns {building id: list of generated tweets}'\n",
    "\n",
    "# Process a single line of metadata\n",
    "def process_metadata(metadata):\n",
    "    data_prompt = format_data_prompt(metadata)\n",
    "    prompt = [\n",
    "        {\"role\": \"system\", \"content\": context_prompt},\n",
    "        {\"role\": \"user\", \"content\": data_prompt}]\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to write results iteratively\n",
    "def write_result(outfile, index, result):\n",
    "    try:\n",
    "        outfile.write(json.dumps({\"index\": index, \"output\": result}, ensure_ascii=False) + \"\\n\")\n",
    "        outfile.flush()  # Immediately save to disk\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing result for index {index}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing buildings: 645it [22:36:04, 64.47s/it] "
     ]
    }
   ],
   "source": [
    "# Start processing from the 4000th row\n",
    "START_ROW = 4280\n",
    "\n",
    "with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as infile, open(OUTPUT_FILE, \"a\", encoding=\"utf-8\") as outfile:\n",
    "    # Skip the first START_ROW - 1 rows\n",
    "    for index, line in enumerate(tqdm(islice(infile, START_ROW - 1, None), desc=\"Processing buildings\"), start=START_ROW):\n",
    "        try:\n",
    "            metadata = json.loads(line)\n",
    "            prompt = process_metadata(metadata)\n",
    "\n",
    "            # Call the text generation pipeline\n",
    "            result = text_pipeline(prompt)\n",
    "            generated_text = result[0][\"generated_text\"]\n",
    "\n",
    "            # Write the generated result with the index\n",
    "            write_result(outfile, index, generated_text)\n",
    "        except Exception as e:\n",
    "            # Write \"failed\" with the exception message\n",
    "            write_result(outfile, index, str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
