{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from itertools import islice\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import math\n",
    "from cleantext import clean\n",
    "import emoji\n",
    "import re\n",
    "import ast\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1360\n"
     ]
    }
   ],
   "source": [
    "print(os.getpid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1360/1122205988.py:2: DtypeWarning: Columns (12,13,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  real_big = pd.read_csv('/mntssd/mnt3/shanshanbai/my_storage_from_qian/Data_Train_Test/first_paper/single_imbalance_noduplicate_test.csv')\n"
     ]
    }
   ],
   "source": [
    "real_small = pd.read_csv('/mntssd/mnt3/shanshanbai/my_storage_from_qian/results/generated tweets/real.csv')\n",
    "real_big = pd.read_csv('/mntssd/mnt3/shanshanbai/my_storage_from_qian/Data_Train_Test/first_paper/single_imbalance_noduplicate_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows from df2 if building_id is in df1\n",
    "real_big_filtered = real_big[~real_big['building_id'].isin(real_small['building_id'])]\n",
    "# Filter rows where language is 'en'\n",
    "real_big_filtered = real_big_filtered[real_big_filtered['language'] == 'en']\n",
    "train = real_big_filtered.original_tweet.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(raw_sentences):\n",
    "    tokenized_sentences = []\n",
    "    for sentence in raw_sentences:\n",
    "        sentence = sentence.lower()\n",
    "        sentence = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", sentence, flags=re.MULTILINE)\n",
    "        sentence = re.sub(r\"@\\w+|#\\w+\", \"\", sentence)\n",
    "        sentence = re.sub(r\"[^a-zA-Z\\s]\", \"\", sentence)\n",
    "        tokens = sentence.split()\n",
    "        tokens = [\"<s>\"] + tokens + [\"</s>\"]\n",
    "        tokenized_sentences.append(tokens)\n",
    "    return tokenized_sentences\n",
    "\n",
    "# Preprocess dataset\n",
    "tokenized_sentences = preprocess_dataset(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_unigram_model(tokenized_sentences):\n",
    "    \"\"\"\n",
    "    Train a unigram language model by counting token frequencies.\n",
    "    Args:\n",
    "        tokenized_sentences (list of list of str): List of tokenized sentences.\n",
    "    Returns:\n",
    "        unigram_counts (dict): Count of each unigram (word).\n",
    "        total_tokens (int): Total number of tokens in the dataset.\n",
    "    \"\"\"\n",
    "    unigram_counts = defaultdict(int)\n",
    "    total_tokens = 0\n",
    "\n",
    "    for sentence in tokenized_sentences:\n",
    "        for token in sentence:\n",
    "            unigram_counts[token] += 1\n",
    "            total_tokens += 1\n",
    "\n",
    "    return unigram_counts, total_tokens\n",
    "\n",
    "# Train unigram model\n",
    "unigram_counts, total_tokens = train_unigram_model(tokenized_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_unigram_probabilities(unigram_counts, total_tokens):\n",
    "    unigram_probabilities = {}\n",
    "    for token, count in unigram_counts.items():\n",
    "        unigram_probabilities[token] = count / total_tokens\n",
    "    return unigram_probabilities\n",
    "\n",
    "# Calculate unigram probabilities\n",
    "unigram_probabilities = calculate_unigram_probabilities(unigram_counts, total_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save n-gram probabilities\n",
    "with open(\"/mntssd/mnt3/shanshanbai/my_storage_from_qian/results/trained_models/unigram_model.json\", \"w\") as f:\n",
    "    json.dump(unigram_probabilities, f)\n",
    "\n",
    "# # Load n-gram probabilities\n",
    "# with open(\"ngram_model.json\", \"r\") as f:\n",
    "#     loaded_ngram_probs = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(dataset, unigram_probs):\n",
    "    total_log_prob = 0\n",
    "    word_count = 0\n",
    "\n",
    "    for sentence in dataset:\n",
    "        tokens = sentence.split()\n",
    "        for token in tokens:\n",
    "            prob = unigram_probs.get(token, 1e-8)  # Handle unseen words with small probability\n",
    "            total_log_prob += math.log10(prob)  # Log probability (base 2)\n",
    "            word_count += 1\n",
    "\n",
    "    if word_count == 0:\n",
    "        return float(\"inf\"), float(\"inf\")  # Avoid division by zero\n",
    "\n",
    "    perplexity = 10 ** (-total_log_prob / word_count)\n",
    "    log_perplexity = math.log10(perplexity)  # Logarithmized perplexity (base 2)\n",
    "\n",
    "    return perplexity, log_perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic Data Perplexity: (31169.41253464588, 4.493728616990799)\n",
      "Real Data Perplexity: (22169.78239835986, 4.345761430432695)\n"
     ]
    }
   ],
   "source": [
    "real = pd.read_csv('/mntssd/mnt3/shanshanbai/my_storage_from_qian/results/generated tweets/real.csv')\n",
    "synthetic = pd.read_csv('/mntssd/mnt3/shanshanbai/my_storage_from_qian/results/generated tweets/synthetic.csv')\n",
    "real['tweets'] = real['tweet_no_url'].apply(ast.literal_eval)\n",
    "synthetic['tweets'] = synthetic['tweets'].apply(ast.literal_eval)\n",
    "# real['tweets'] = real['tweet_no_url'].apply(ast.literal_eval)\n",
    "# synthetic['tweets'] = synthetic['tweets'].apply(ast.literal_eval)\n",
    "# real = real[real['language'] == 'en']\n",
    "# synthetic = synthetic[synthetic['language'] == 'en']\n",
    "\n",
    "\n",
    "real_data = [tweet for sublist in real['tweets'] for tweet in sublist]\n",
    "synthetic_data = [tweet for sublist in synthetic['tweets'] for tweet in sublist]\n",
    "\n",
    "def clean_dataset(raw_sentences):\n",
    "    tokenized_sentences = []\n",
    "    for sentence in raw_sentences:\n",
    "        sentence = sentence.lower()\n",
    "        sentence = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", sentence, flags=re.MULTILINE)\n",
    "        sentence = re.sub(r\"@\\w+|#\\w+\", \"\", sentence)\n",
    "        sentence = re.sub(r\"[^a-zA-Z\\s]\", \"\", sentence)\n",
    "        tokenized_sentences.append(sentence)\n",
    "    return tokenized_sentences\n",
    "\n",
    "# Preprocess dataset\n",
    "real_data = clean_dataset(real_data)\n",
    "synthetic_data = clean_dataset(synthetic_data)\n",
    "\n",
    "# Calculate perplexity\n",
    "smoothed_probs = calculate_unigram_probabilities(unigram_counts, total_tokens)\n",
    "synthetic_perplexity = calculate_perplexity(synthetic_data, smoothed_probs)\n",
    "real_perplexity = calculate_perplexity(real_data, smoothed_probs)\n",
    "\n",
    "print(f\"Synthetic Data Perplexity: {synthetic_perplexity}\")\n",
    "print(f\"Real Data Perplexity: {real_perplexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
