{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3965\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from copy import deepcopy\n",
    "from transformers import (AutoTokenizer,\n",
    "                         BertForSequenceClassification,\n",
    "                         Trainer,\n",
    "                         TrainingArguments,\n",
    "                         TrainerCallback,\n",
    "                         get_linear_schedule_with_warmup\n",
    "                          )\n",
    "from sklearn.utils import resample\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\n",
    "# import wandb\n",
    "# wandb.login()\n",
    "# wandb.init(project=\"synthetic_tweets\")\n",
    "import ast\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "print(os.getpid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweet_no_url\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweet_no_url\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(ast\u001b[38;5;241m.\u001b[39mliteral_eval) \u001b[38;5;66;03m# tweet_no_url\u001b[39;00m\n\u001b[1;32m      2\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweets_concatenated\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweet_no_url\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([item \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m x \u001b[38;5;28;01mif\u001b[39;00m item]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# df_synthetic = pd.read_json(\"/mntssd/mnt3/shanshanbai/nlpinearthobservation/synthetic_data/6000/merged_data.jsonl\", lines=True)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# df_synthetic['tweets_concatenated'] = df['tweets'].apply(lambda x: ' '.join([item for item in x if item]) if isinstance(x, list) else '')\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# df_synthetic['mapped_class'] = ['residential'] * 3000 + ['commercial'] * (len(df) - 3000)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "df['tweet_no_url'] = df['tweet_no_url'].apply(ast.literal_eval) # tweet_no_url\n",
    "df['tweets_concatenated'] = df['tweet_no_url'].apply(lambda x: ' '.join([item for item in x if item]) if isinstance(x, list) else '')\n",
    "\n",
    "# df_synthetic = pd.read_json(\"/mntssd/mnt3/shanshanbai/nlpinearthobservation/synthetic_data/6000/merged_data.jsonl\", lines=True)\n",
    "# df_synthetic['tweets_concatenated'] = df['tweets'].apply(lambda x: ' '.join([item for item in x if item]) if isinstance(x, list) else '')\n",
    "# df_synthetic['mapped_class'] = ['residential'] * 3000 + ['commercial'] * (len(df) - 3000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Check class distribution\n",
    "class_counts = df['mapped_class'].value_counts()\n",
    "print(\"Class distribution before undersampling:\\n\", class_counts)\n",
    "\n",
    "# # Step 2: Find the smaller class size\n",
    "# min_class_size = 3000\n",
    "\n",
    "# # Step 3: Undersample the larger class\n",
    "# df = (\n",
    "#     df.groupby('mapped_class')  # Group by the class\n",
    "#     .apply(lambda x: x.sample(n=min_class_size, random_state=42))  # Sample from each group\n",
    "#     .reset_index(drop=True)  # Reset the index\n",
    "# )\n",
    "\n",
    "# # Step 4: Check class distribution after undersampling\n",
    "# balanced_class_counts = df['mapped_class'].value_counts()\n",
    "# print(\"Class distribution after undersampling:\\n\", balanced_class_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['language'].apply(len).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(set(item for sublist in df['language'] for item in sublist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.city.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, split into train + eval and test\n",
    "X = df['tweets_concatenated'].values.tolist()\n",
    "y = df['mapped_class'].values.tolist()\n",
    "# First, split into train + eval and test\n",
    "X_train_eval, X_test, y_train_eval, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# Then, split train + eval into train and eval\n",
    "X_train, X_eval, y_train, y_eval= train_test_split(X_train_eval, y_train_eval, test_size=0.2, random_state=0)  # 0.25 * 0.8 = 0.2\n",
    "\n",
    "print(len(X_train))\n",
    "print(len(X_eval))\n",
    "print(len(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert text to numerical features\n",
    "# vectorizer = TfidfVectorizer()\n",
    "# X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "# X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# # Train Naive Bayes\n",
    "# model = MultinomialNB()\n",
    "# model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# # Predict and evaluate\n",
    "# y_pred = model.predict(X_test_tfidf)\n",
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECK_POINT = \"google-bert/bert-base-multilingual-cased\"\n",
    "CACHE_DIR = \"/mntssd/mnt3/shanshanbai/my_storage_from_qian/.cache/huggingface/hub\"  # Specify your custom directory\n",
    "\n",
    "# Load tokenizer and model with the specified cache directory\n",
    "tokenizer = AutoTokenizer.from_pretrained(CHECK_POINT, cache_dir=CACHE_DIR)\n",
    "# model = BertForSequenceClassification.from_pretrained(CHECK_POINT, cache_dir=CACHE_DIR, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_and_labels(X, y):\n",
    "    sentences = X\n",
    "    label_str = y\n",
    "    labels = [0 if label == \"residential\" else 1 for label in label_str]\n",
    "    encodings = tokenizer(sentences, truncation=True, padding=True, return_tensors=\"pt\") # , is_split_into_words=True\n",
    "    return encodings, labels\n",
    "\n",
    "train_encodings, train_labels = encoding_and_labels(X_train, y_train)\n",
    "eval_encodings, eval_labels = encoding_and_labels(X_eval, y_eval)\n",
    "test_encodings, test_labels = encoding_and_labels(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "train_dataset = TwitterDataset(train_encodings, train_labels)\n",
    "eval_dataset = TwitterDataset(eval_encodings, eval_labels)\n",
    "test_dataset = TwitterDataset(test_encodings, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_prediction):\n",
    "    predictions = eval_prediction.predictions.argmax(-1)\n",
    "    labels = eval_prediction.label_ids\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='macro')\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    metrics = {'accuracy': acc,\n",
    "               'precision': precision,\n",
    "               'recall': recall,\n",
    "               'f1': f1}\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"/mntssd/mnt3/shanshanbai/my_storage_from_qian/results/generated tweets/output_real\"\n",
    "LOGGING_DIR =\"/mntssd/mnt3/shanshanbai/my_storage_from_qian/results/generated tweets/output_real\"\n",
    "NUM_EPOCHS = 5\n",
    "TRAIN_BATCH_SIZE = 32 #8\n",
    "EVAL_BATCH_SIZE = 32 #32\n",
    "WARMUP_RATIO = 0.01 # 200\n",
    "WEIGHT_DECAY = 0.01\n",
    "LOGGING_STEPS = 100 # 1000\n",
    "LEARNING_RATE = 5e-6\n",
    "\n",
    "# Calculate total training steps\n",
    "total_train_steps = (len(train_labels) // TRAIN_BATCH_SIZE) * NUM_EPOCHS\n",
    "print(total_train_steps)\n",
    "\n",
    "# Calculate the saving steps to be aligned with logging steps\n",
    "SAVING_STEPS = LOGGING_STEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "    warmup_ratio= WARMUP_RATIO,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    logging_dir=LOGGING_DIR,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    save_steps=SAVING_STEPS,  # Set saving steps to align with logging steps\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    metric_for_best_model=\"f1\"\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,  #{'train':train_dataset,'eval':eval_dataset},\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_prediction):\n",
    "    predictions = eval_prediction.predictions.argmax(-1)\n",
    "    labels = eval_prediction.label_ids\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='macro')\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    metrics = {'accuracy': acc,\n",
    "               'precision': precision,\n",
    "               'recall': recall,\n",
    "               'f1': f1}\n",
    "    \n",
    "    # convert label_id to label name\n",
    "    mapping = {0: \"residential\", 1: \"commercial\"} \n",
    "    labels = [mapping[label_id] for label_id in labels]\n",
    "    predictions = [mapping[prediction] for prediction in predictions]\n",
    "    \n",
    "    # create classification report\n",
    "    report = classification_report(labels, predictions, zero_division=0)\n",
    "    print(\"Classification Report:\\n\\n\\n\", report)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_trained = \"/mntssd/mnt3/shanshanbai/my_storage_from_qian/results/generated tweets/output_real/checkpoint-600\"  # Specify your custom directory\n",
    "\n",
    "# model = BertForSequenceClassification.from_pretrained(model_trained, cache_dir=CACHE_DIR, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "test_args = TrainingArguments(\n",
    "    output_dir='/mntssd/mnt3/shanshanbai/my_storage_from_qian/results/generated tweets/output_synthetic',\n",
    "    do_train=False,\n",
    "    do_predict=True,\n",
    "    per_device_eval_batch_size=32,   \n",
    "    dataloader_drop_last=False,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=test_args,\n",
    "    compute_metrics=compute_metrics,\n",
    ") \n",
    "\n",
    "test_results = trainer.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract predicted class indices\n",
    "# y_pred = np.argmax(test_results.predictions, axis=1)\n",
    "# # Extract true labels from test_dataset (assuming test_dataset has labels)\n",
    "# y_test = test_dataset.labels # Adjust if labels are stored differently\n",
    "\n",
    "# # Find indices where predictions do not match actual labels\n",
    "# misclassified_indices = np.where(y_test != y_pred)[0]\n",
    "\n",
    "# # Create a DataFrame of misclassified samples\n",
    "# misclassified_samples = pd.DataFrame({\n",
    "#     'Text': [X_test[i] for i in misclassified_indices],\n",
    "#     'Actual Label': [y_test[i] for i in misclassified_indices],\n",
    "#     'Predicted Label': [y_pred[i] for i in misclassified_indices]\n",
    "# })\n",
    "# misclassified_samples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
