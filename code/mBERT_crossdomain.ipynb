{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3965\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from copy import deepcopy\n",
    "from transformers import (AutoTokenizer,\n",
    "                         BertForSequenceClassification,\n",
    "                         Trainer,\n",
    "                         TrainingArguments,\n",
    "                         TrainerCallback,\n",
    "                         get_linear_schedule_with_warmup\n",
    "                          )\n",
    "from sklearn.utils import resample\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\n",
    "# import wandb\n",
    "# wandb.login()\n",
    "# wandb.init(project=\"synthetic_tweets\")\n",
    "import ast\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "print(os.getpid())\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>building_id</th>\n",
       "      <th>building_city</th>\n",
       "      <th>building_tags</th>\n",
       "      <th>building_name</th>\n",
       "      <th>tweet_language_distribution</th>\n",
       "      <th>tweets</th>\n",
       "      <th>tweets_concatenated</th>\n",
       "      <th>mapped_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31061</td>\n",
       "      <td>CapeTown</td>\n",
       "      <td>dormitory</td>\n",
       "      <td>Kopano</td>\n",
       "      <td>[English, English, English, English, English]</td>\n",
       "      <td>[Midterm exams are looming and I'm stuck here ...</td>\n",
       "      <td>Midterm exams are looming and I'm stuck here h...</td>\n",
       "      <td>residential</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31073</td>\n",
       "      <td>CapeTown</td>\n",
       "      <td>dormitory</td>\n",
       "      <td>Baxter Hall</td>\n",
       "      <td>[English, English, English, English, English]</td>\n",
       "      <td>[Cramming for exams is way more bearable when ...</td>\n",
       "      <td>Cramming for exams is way more bearable when y...</td>\n",
       "      <td>residential</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31079</td>\n",
       "      <td>CapeTown</td>\n",
       "      <td>dormitory</td>\n",
       "      <td>Gra√ßa Machel Hall</td>\n",
       "      <td>[English, English, English, English, Polish]</td>\n",
       "      <td>[Morning views from Gra√ßa Mached don't get muc...</td>\n",
       "      <td>Morning views from Gra√ßa Mached don't get much...</td>\n",
       "      <td>residential</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>57201</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>apartments</td>\n",
       "      <td>Doubletree By Hilton Berlin Ku'Damm</td>\n",
       "      <td>[German]</td>\n",
       "      <td>[Eine Woche im Doubletree und ich f√ºhle mich w...</td>\n",
       "      <td>Eine Woche im Doubletree und ich f√ºhle mich wi...</td>\n",
       "      <td>residential</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000</th>\n",
       "      <td>103615</td>\n",
       "      <td>Munich</td>\n",
       "      <td>office</td>\n",
       "      <td>Wacker Chemie Ag</td>\n",
       "      <td>[German, German, German, German, English]</td>\n",
       "      <td>[Bei so einem Wetter wie heute freue ich mich ...</td>\n",
       "      <td>Bei so einem Wetter wie heute freue ich mich b...</td>\n",
       "      <td>commercial</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      building_id building_city building_tags  \\\n",
       "0           31061      CapeTown     dormitory   \n",
       "1           31073      CapeTown     dormitory   \n",
       "2           31079      CapeTown     dormitory   \n",
       "3           57201        Berlin    apartments   \n",
       "3000       103615        Munich        office   \n",
       "\n",
       "                            building_name  \\\n",
       "0                                  Kopano   \n",
       "1                             Baxter Hall   \n",
       "2                       Gra√ßa Machel Hall   \n",
       "3     Doubletree By Hilton Berlin Ku'Damm   \n",
       "3000                     Wacker Chemie Ag   \n",
       "\n",
       "                        tweet_language_distribution  \\\n",
       "0     [English, English, English, English, English]   \n",
       "1     [English, English, English, English, English]   \n",
       "2      [English, English, English, English, Polish]   \n",
       "3                                          [German]   \n",
       "3000      [German, German, German, German, English]   \n",
       "\n",
       "                                                 tweets  \\\n",
       "0     [Midterm exams are looming and I'm stuck here ...   \n",
       "1     [Cramming for exams is way more bearable when ...   \n",
       "2     [Morning views from Gra√ßa Mached don't get muc...   \n",
       "3     [Eine Woche im Doubletree und ich f√ºhle mich w...   \n",
       "3000  [Bei so einem Wetter wie heute freue ich mich ...   \n",
       "\n",
       "                                    tweets_concatenated mapped_class  \n",
       "0     Midterm exams are looming and I'm stuck here h...  residential  \n",
       "1     Cramming for exams is way more bearable when y...  residential  \n",
       "2     Morning views from Gra√ßa Mached don't get much...  residential  \n",
       "3     Eine Woche im Doubletree und ich f√ºhle mich wi...  residential  \n",
       "3000  Bei so einem Wetter wie heute freue ich mich b...   commercial  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_synthetic = pd.read_json(\"/mntssd/mnt3/shanshanbai/nlpinearthobservation/synthetic_data/6000/merged_data.jsonl\", lines=True)\n",
    "df_synthetic['tweets_concatenated'] = df_synthetic['tweets'].apply(lambda x: ' '.join([item for item in x if item]) if isinstance(x, list) else '')\n",
    "df_synthetic['mapped_class'] = ['residential'] * 3000 + ['commercial'] * (len(df_synthetic) - 3000)\n",
    "df_synthetic.sort_values(by='building_id', inplace=True)\n",
    "df_synthetic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>building_id</th>\n",
       "      <th>mapped_class</th>\n",
       "      <th>language</th>\n",
       "      <th>city</th>\n",
       "      <th>tweet_no_url</th>\n",
       "      <th>tweets_concatenated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31061</td>\n",
       "      <td>residential</td>\n",
       "      <td>['en', 'en', 'en', 'en', 'in', 'en', 'en', 'lt...</td>\n",
       "      <td>CapeTown</td>\n",
       "      <td>[Saturday mornings will never be the same agai...</td>\n",
       "      <td>Saturday mornings will never be the same again...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31073</td>\n",
       "      <td>residential</td>\n",
       "      <td>['en', 'en', 'en', 'en', 'en']</td>\n",
       "      <td>CapeTown</td>\n",
       "      <td>[Just posted a photo @ Baxter Hall , Happiness...</td>\n",
       "      <td>Just posted a photo @ Baxter Hall  Happiness i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31079</td>\n",
       "      <td>residential</td>\n",
       "      <td>['en', 'en', 'pl', 'en', 'en']</td>\n",
       "      <td>CapeTown</td>\n",
       "      <td>[Feels good to be back. üò≠üôè, Students, did you ...</td>\n",
       "      <td>Feels good to be back. üò≠üôè Students, did you mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>57201</td>\n",
       "      <td>residential</td>\n",
       "      <td>['de']</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>[Kunstgalerie Robert Grunenberg ]</td>\n",
       "      <td>Kunstgalerie Robert Grunenberg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103615</td>\n",
       "      <td>commercial</td>\n",
       "      <td>['de', 'en', 'de', 'de']</td>\n",
       "      <td>Munich</td>\n",
       "      <td>[I'm at Neuperlach in M√ºnchen , Bradley Co Sch...</td>\n",
       "      <td>I'm at Neuperlach in M√ºnchen  Bradley Co Schoo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   building_id mapped_class  \\\n",
       "0        31061  residential   \n",
       "1        31073  residential   \n",
       "2        31079  residential   \n",
       "3        57201  residential   \n",
       "4       103615   commercial   \n",
       "\n",
       "                                            language      city  \\\n",
       "0  ['en', 'en', 'en', 'en', 'in', 'en', 'en', 'lt...  CapeTown   \n",
       "1                     ['en', 'en', 'en', 'en', 'en']  CapeTown   \n",
       "2                     ['en', 'en', 'pl', 'en', 'en']  CapeTown   \n",
       "3                                             ['de']    Berlin   \n",
       "4                           ['de', 'en', 'de', 'de']    Munich   \n",
       "\n",
       "                                        tweet_no_url  \\\n",
       "0  [Saturday mornings will never be the same agai...   \n",
       "1  [Just posted a photo @ Baxter Hall , Happiness...   \n",
       "2  [Feels good to be back. üò≠üôè, Students, did you ...   \n",
       "3                  [Kunstgalerie Robert Grunenberg ]   \n",
       "4  [I'm at Neuperlach in M√ºnchen , Bradley Co Sch...   \n",
       "\n",
       "                                 tweets_concatenated  \n",
       "0  Saturday mornings will never be the same again...  \n",
       "1  Just posted a photo @ Baxter Hall  Happiness i...  \n",
       "2  Feels good to be back. üò≠üôè Students, did you mi...  \n",
       "3                    Kunstgalerie Robert Grunenberg   \n",
       "4  I'm at Neuperlach in M√ºnchen  Bradley Co Schoo...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_real = pd.read_csv('/mntssd/mnt3/shanshanbai/my_storage_from_qian/results/generated tweets/real.csv')\n",
    "df_real = df_real[df_real['building_id'].isin(df_synthetic['building_id'])]\n",
    "df_real['tweet_no_url'] = df_real['tweet_no_url'].apply(ast.literal_eval)\n",
    "df_real['tweets_concatenated'] = df_real['tweet_no_url'].apply(lambda x: ' '.join([item for item in x if item]) if isinstance(x, list) else '')\n",
    "df_real.sort_values(by='building_id', inplace=True)\n",
    "df_real.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_real = pd.read_csv('/mntssd/mnt3/shanshanbai/my_storage_from_qian/Data_Train_Test/first_paper/concatenated_imbalance_nobrokenimage_train_multilingual.csv')\n",
    "# df_real = df_real[~df_real['building_id'].isin(df_synthetic['building_id'])]\n",
    "# residential_sample = df_real[df_real['mapped_class'] == 'residential'].sample(n=1000, random_state=42)\n",
    "# commercial_sample = df_real[df_real['mapped_class'] == 'commercial'].sample(n=1000, random_state=42)\n",
    "\n",
    "# # Combine both samples into one DataFrame\n",
    "# df_real = pd.concat([residential_sample, commercial_sample])\n",
    "\n",
    "# # Function to remove URLs\n",
    "# def remove_urls(text):\n",
    "#     url_pattern = r'http[s]?://\\S+|www\\.\\S+'  # Matches URLs\n",
    "#     return re.sub(url_pattern, '', text)\n",
    "\n",
    "# # Apply function to the column\n",
    "# df_real['concatenated_original'] = df_real['concatenated_original'].apply(remove_urls)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # train on synthetic and test on real\n",
    "\n",
    "# # # First, split into train + eval and test\n",
    "# # X_synthetic = df_synthetic['tweets_concatenated'].values.tolist()\n",
    "# # X_real = df_real['tweets_concatenated'].values.tolist()\n",
    "# # y_synthetic = df_synthetic['mapped_class'].values.tolist()\n",
    "# # y_real = df_real['mapped_class'].values.tolist()\n",
    "# # # First, split into train + eval and test\n",
    "# # X_train_eval, X_test, y_train_eval, y_test = train_test_split(X_synthetic, y_synthetic, test_size=0.2, random_state=5)\n",
    "# # # Then, split train + eval into train and eval\n",
    "# # X_train, _, y_train, _= train_test_split(X_train_eval, y_train_eval, test_size=0.2, random_state=5)  # 0.25 * 0.8 = 0.2\n",
    "\n",
    "# # # _, X_test, _, y_test = train_test_split(X_real, y_real, test_size=0.2, random_state=4)\n",
    "\n",
    "\n",
    "# # # print(len(X_train))\n",
    "# # # print(len(X_eval))\n",
    "# # # print(len(X_test))\n",
    "\n",
    "\n",
    "# X_train = df_synthetic['tweets_concatenated'].values.tolist()\n",
    "# X_test = df_real['tweets_concatenated'].values.tolist()\n",
    "# y_train = df_synthetic['mapped_class'].values.tolist()\n",
    "# y_test = df_real['mapped_class'].values.tolist()\n",
    "\n",
    "# X_train_eval, _, y_train_eval, _ = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "# X_train, _, y_train, _= train_test_split(X_train_eval, y_train_eval, test_size=0.2, random_state=0)  # 0.25 * 0.8 = 0.2\n",
    "# # X_test = df_real['concatenated_original'].values.tolist()\n",
    "# # y_test = df_real['mapped_class'].values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, split into train + eval and test\n",
    "X_synthetic = df_synthetic['tweets_concatenated'].values.tolist()\n",
    "X_real = df_real['tweets_concatenated'].values.tolist()\n",
    "y_synthetic = df_synthetic['mapped_class'].values.tolist()\n",
    "y_real = df_real['mapped_class'].values.tolist()\n",
    "# First, split into train + eval and test\n",
    "X_train_eval, _, y_train_eval, _ = train_test_split(X_synthetic, y_synthetic, test_size=0.2, random_state=0)\n",
    "\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(X_train_eval, y_train_eval, test_size=0.2, random_state=0)\n",
    "_, X_test, _, y_test = train_test_split(X_real, y_real, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "# # Convert text to numerical features\n",
    "# vectorizer = TfidfVectorizer()\n",
    "# X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "# X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# # Train Naive Bayes\n",
    "# model = MultinomialNB()\n",
    "# model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# # Predict and evaluate\n",
    "# y_pred = model.predict(X_test_tfidf)\n",
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "CHECK_POINT = \"google-bert/bert-base-multilingual-cased\"\n",
    "CACHE_DIR = \"/mntssd/mnt3/shanshanbai/my_storage_from_qian/.cache/huggingface/hub\"  # Specify your custom directory\n",
    "\n",
    "# Load tokenizer and model with the specified cache directory\n",
    "tokenizer = AutoTokenizer.from_pretrained(CHECK_POINT)\n",
    "model = BertForSequenceClassification.from_pretrained(CHECK_POINT, cache_dir=CACHE_DIR, num_labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_and_labels(X, y):\n",
    "    sentences = X\n",
    "    label_str = y\n",
    "    labels = [0 if label == \"residential\" else 1 for label in label_str]\n",
    "    encodings = tokenizer(sentences, truncation=True, padding=True, return_tensors=\"pt\") # , is_split_into_words=True\n",
    "    return encodings, labels\n",
    "\n",
    "train_encodings, train_labels = encoding_and_labels(X_train, y_train)\n",
    "eval_encodings, eval_labels = encoding_and_labels(X_eval, y_eval)\n",
    "test_encodings, test_labels = encoding_and_labels(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "train_dataset = TwitterDataset(train_encodings, train_labels)\n",
    "eval_dataset = TwitterDataset(eval_encodings, eval_labels)\n",
    "test_dataset = TwitterDataset(test_encodings, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_prediction):\n",
    "    predictions = eval_prediction.predictions.argmax(-1)\n",
    "    labels = eval_prediction.label_ids\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='macro')\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    metrics = {'accuracy': acc,\n",
    "               'precision': precision,\n",
    "               'recall': recall,\n",
    "               'f1': f1}\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_DIR = \"/mntssd/mnt3/shanshanbai/my_storage_from_qian/results/generated tweets/output_synthetic\"\n",
    "LOGGING_DIR =\"/mntssd/mnt3/shanshanbai/my_storage_from_qian/results/generated tweets/output_synthetic\"\n",
    "NUM_EPOCHS = 5\n",
    "TRAIN_BATCH_SIZE = 32 #8\n",
    "EVAL_BATCH_SIZE = 32 #32\n",
    "WARMUP_RATIO = 0.01 # 200\n",
    "WEIGHT_DECAY = 0.01\n",
    "LOGGING_STEPS = 100 # 1000\n",
    "LEARNING_RATE = 5e-6\n",
    "\n",
    "# Calculate total training steps\n",
    "total_train_steps = (len(train_labels) // TRAIN_BATCH_SIZE) * NUM_EPOCHS\n",
    "print(total_train_steps)\n",
    "\n",
    "# Calculate the saving steps to be aligned with logging steps\n",
    "SAVING_STEPS = LOGGING_STEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshanshan-bai\u001b[0m (\u001b[33mnlp-in-earth-observation\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20250209_083831-zpeidqeh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nlp-in-earth-observation/huggingface/runs/zpeidqeh' target=\"_blank\">/mntssd/mnt3/shanshanbai/my_storage_from_qian/results/generated tweets/output_synthetic</a></strong> to <a href='https://wandb.ai/nlp-in-earth-observation/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nlp-in-earth-observation/huggingface' target=\"_blank\">https://wandb.ai/nlp-in-earth-observation/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nlp-in-earth-observation/huggingface/runs/zpeidqeh' target=\"_blank\">https://wandb.ai/nlp-in-earth-observation/huggingface/runs/zpeidqeh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3965/1560154188.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='401' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [401/600 1:16:45 < 38:16, 0.09 it/s, Epoch 3.33/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.475100</td>\n",
       "      <td>0.188659</td>\n",
       "      <td>0.941667</td>\n",
       "      <td>0.941667</td>\n",
       "      <td>0.941858</td>\n",
       "      <td>0.941660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.146600</td>\n",
       "      <td>0.100482</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.974965</td>\n",
       "      <td>0.975163</td>\n",
       "      <td>0.974996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.110300</td>\n",
       "      <td>0.085762</td>\n",
       "      <td>0.976042</td>\n",
       "      <td>0.976376</td>\n",
       "      <td>0.975836</td>\n",
       "      <td>0.976020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>0.081535</td>\n",
       "      <td>0.980208</td>\n",
       "      <td>0.980442</td>\n",
       "      <td>0.980048</td>\n",
       "      <td>0.980193</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3965/1560154188.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/tmp/ipykernel_3965/1560154188.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/tmp/ipykernel_3965/1560154188.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "    warmup_ratio= WARMUP_RATIO,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    logging_dir=LOGGING_DIR,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    save_steps=SAVING_STEPS,  # Set saving steps to align with logging steps\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    metric_for_best_model=\"f1\"\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,  #{'train':train_dataset,'eval':eval_dataset},\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_trained = \"/mntssd/mnt3/shanshanbai/my_storage_from_qian/results/generated tweets/output_synthetic/checkpoint-100\"  # Specify your custom directory\n",
    "\n",
    "# model = BertForSequenceClassification.from_pretrained(model_trained, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_prediction):\n",
    "    predictions = eval_prediction.predictions.argmax(-1)\n",
    "    labels = eval_prediction.label_ids\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='macro')\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    metrics = {'accuracy': acc,\n",
    "               'precision': precision,\n",
    "               'recall': recall,\n",
    "               'f1': f1}\n",
    "    \n",
    "    # convert label_id to label name\n",
    "    mapping = {0: \"residential\", 1: \"commercial\"} \n",
    "    labels = [mapping[label_id] for label_id in labels]\n",
    "    predictions = [mapping[prediction] for prediction in predictions]\n",
    "    \n",
    "    # create classification report\n",
    "    report = classification_report(labels, predictions, zero_division=0)\n",
    "    print(\"Classification Report:\\n\\n\\n\", report)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "test_args = TrainingArguments(\n",
    "    output_dir='/mntssd/mnt3/shanshanbai/my_storage_from_qian/results/generated tweets/output_synthetic',\n",
    "    do_train=False,\n",
    "    do_predict=True,\n",
    "    per_device_eval_batch_size=32,   \n",
    "    dataloader_drop_last=False,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=test_args,\n",
    "    compute_metrics=compute_metrics,\n",
    ") \n",
    "\n",
    "test_results = trainer.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract predicted class indices\n",
    "# y_pred = np.argmax(test_results.predictions, axis=1)\n",
    "# # Extract true labels from test_dataset (assuming test_dataset has labels)\n",
    "# y_test = test_dataset.labels # Adjust if labels are stored differently\n",
    "\n",
    "# # Find indices where predictions do not match actual labels\n",
    "# misclassified_indices = np.where(y_test != y_pred)[0]\n",
    "\n",
    "# # Create a DataFrame of misclassified samples\n",
    "# misclassified_samples = pd.DataFrame({\n",
    "#     'Text': [X_test[i] for i in misclassified_indices],\n",
    "#     'Actual Label': [y_test[i] for i in misclassified_indices],\n",
    "#     'Predicted Label': [y_pred[i] for i in misclassified_indices]\n",
    "# })\n",
    "# misclassified_samples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
