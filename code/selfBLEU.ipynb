{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1119\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import os\n",
    "print(os.getpid())\n",
    "import json\n",
    "from sacrebleu.metrics import BLEU\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "import random\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentences(sentences):\n",
    "    \"\"\"\n",
    "    Tokenize sentences using Hugging Face's Tokenizers for efficiency.\n",
    "    \"\"\"\n",
    "    from transformers import AutoTokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-multilingual-cased\", cache_dir=\"/mntssd/mnt3/shanshanbai/my_storage_from_qian/.cache/huggingface/hub\")\n",
    "    return [tokenizer.tokenize(tweet) for tweet in all_tweets]\n",
    "\n",
    "# def preprocess_sentences(sentences):\n",
    "#     \"\"\"\n",
    "#     Tokenize sentences using a whitespace tokenizer.\n",
    "#     \"\"\"\n",
    "#     return [sentence.split() for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ngram_overlap(candidate, references, n=4):\n",
    "    candidate_ngrams = Counter(ngrams(candidate, n))\n",
    "    reference_ngrams = Counter()\n",
    "    for ref in references:\n",
    "        reference_ngrams.update(ngrams(ref, n))\n",
    "    overlap = sum((candidate_ngrams & reference_ngrams).values())\n",
    "    total_ngrams = sum(candidate_ngrams.values())\n",
    "    return overlap / total_ngrams if total_ngrams > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_references(tokenized_sentences, sample_size):\n",
    "    return [\n",
    "        random.sample(tokenized_sentences[:i] + tokenized_sentences[i + 1:], min(sample_size, len(tokenized_sentences) - 1))\n",
    "        for i in range(len(tokenized_sentences))\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_self_bleu_with_precomputed(index, tokenized_sentences, precomputed_references, n=4):\n",
    "    \"\"\"\n",
    "    Calculate Self-BLEU score for a single sentence using precomputed references.\n",
    "    \"\"\"\n",
    "    candidate = tokenized_sentences[index]\n",
    "    references = precomputed_references[index]\n",
    "    bleu = BLEU(effective_order=True)\n",
    "    return bleu.corpus_score([' '.join(candidate)], [[' '.join(ref)] for ref in references]).score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_self_bleu_optimized(tokenized_sentences, precomputed_references, n=4):\n",
    "    \"\"\"\n",
    "    Compute Self-BLEU score in parallel using precomputed references.\n",
    "    \"\"\"\n",
    "    with mp.Pool(processes=min(mp.cpu_count(), 8)) as pool:\n",
    "        results = list(\n",
    "            tqdm(\n",
    "                pool.starmap(\n",
    "                    calculate_self_bleu_with_precomputed,\n",
    "                    [(i, tokenized_sentences, precomputed_references, n) for i in range(len(tokenized_sentences))]\n",
    "                ),\n",
    "                total=len(tokenized_sentences),\n",
    "                desc=\"Calculating Self-BLEU\"\n",
    "            )\n",
    "        )\n",
    "    return sum(results) / len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "# df = pd.read_csv('/mntssd/mnt3/shanshanbai/nlpinearthobservation/synthetic_data/data/synthetic.csv')\n",
    "df = pd.read_csv('/mntssd/mnt3/shanshanbai/my_storage_from_qian/results/generated tweets/real.csv')\n",
    "\n",
    "# # Undersample the dataset for balanced class distribution\n",
    "# min_class_size = 3000\n",
    "# df = (\n",
    "#     df.groupby('mapped_class')\n",
    "#     .apply(lambda x: x.sample(n=min_class_size, random_state=42))\n",
    "#     .reset_index(drop=True)\n",
    "# )\n",
    "\n",
    "# print(\"Class distribution after undersampling:\\n\", df['mapped_class'].value_counts())\n",
    "\n",
    "# Process tweets\n",
    "df['tweets'] = df['tweet_no_url'].apply(ast.literal_eval) # tweet_no_url\n",
    "all_tweets = [tweet for sublist in df['tweets'] for tweet in sublist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25747"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_tweets = preprocess_sentences(all_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomputing references...\n",
      "Calculating Self-BLEU...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Calculating Self-BLEU: 100%|██████████| 25747/25747 [00:00<00:00, 1827192.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-BLEU Score: 46.7727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Precompute references\n",
    "print(\"Precomputing references...\")\n",
    "precomputed_references = precompute_references(tokenized_tweets, sample_size=1000)\n",
    "\n",
    "# Compute Self-BLEU\n",
    "print(\"Calculating Self-BLEU...\")\n",
    "self_bleu_score = compute_self_bleu_optimized(tokenized_tweets, precomputed_references, n=4)\n",
    "\n",
    "print(f\"Self-BLEU Score: {self_bleu_score:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
