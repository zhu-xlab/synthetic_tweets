{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "print(f\"Process ID: {os.getpid()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get {} out\n",
    "\n",
    "# Input and output file paths\n",
    "input_file = \"/mntssd/mnt3/shanshanbai/my_storage_from_qian/results/generated tweets/tweets_better_6000 copy.jsonl\"\n",
    "output_file = \"/mntssd/mnt3/shanshanbai/nlpinearthobservation/synthetic_data/6000/generated_tweets_residential_filtered.jsonl\"\n",
    "invalid_lines_file = \"/mntssd/mnt3/shanshanbai/nlpinearthobservation/synthetic_data/6000/error_row.jsonl\"\n",
    "\n",
    "\n",
    "# Process the file with better error handling\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as infile, \\\n",
    "     open(output_file, \"w\", encoding=\"utf-8\") as outfile, \\\n",
    "     open(invalid_lines_file, \"w\", encoding=\"utf-8\") as errorfile:\n",
    "    \n",
    "    for line_number, line in enumerate(infile, start=1):\n",
    "        try:\n",
    "            # Parse each line as JSON\n",
    "            data = json.loads(line.strip())\n",
    "            # Get the \"output\" field if it exists\n",
    "            output = data.get(\"output\", \"\")\n",
    "            \n",
    "            if output:\n",
    "                # Decode the Unicode characters in the output\n",
    "                # decoded_output = json.loads(output)\n",
    "                outfile.write(json.dumps(output, ensure_ascii=False) + \"\\n\")\n",
    "            else:\n",
    "                # Log lines where `output` is missing\n",
    "                print(f\"Line {line_number}: Missing 'output' field\")\n",
    "                errorfile.write(f\"Line {line_number}: Missing 'output' field\\n\")\n",
    "        except json.JSONDecodeError as e:\n",
    "            # Log invalid JSON lines\n",
    "            print(f\"Skipping line {line_number} due to JSONDecodeError: {e}\")\n",
    "            errorfile.write(f\"Line {line_number}: {line.strip()} | Error: {e}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracts valid JSON-like content from each line\n",
    "\n",
    "input_file = \"/mntssd/mnt3/shanshanbai/nlpinearthobservation/synthetic_data/6000/generated_tweets_residential_filtered.jsonl\"\n",
    "output_file = \"/mntssd/mnt3/shanshanbai/nlpinearthobservation/synthetic_data/6000/temp.jsonl\"\n",
    "\n",
    "\n",
    "# Process the file line by line\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as infile, open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for line_number, line in enumerate(infile, start=1):\n",
    "        original_line = line.strip()  # Remove leading and trailing whitespace\n",
    "        match = re.search(r\"{.*?}\", original_line, re.DOTALL)\n",
    "        \n",
    "        if match:\n",
    "            # If a match is found, write the cleaned content\n",
    "            cleaned_text = match.group()\n",
    "            outfile.write(cleaned_text + \"\\n\")\n",
    "        else:\n",
    "            # If no match, append `]}\"` and retry\n",
    "            fixed_line = original_line[:-2] + '}\"'  # original_line + \"]}\\\"\"\n",
    "            match_retry = re.search(r\"{.*?}\", fixed_line, re.DOTALL)\n",
    "            \n",
    "            if match_retry:\n",
    "                # If a match is found after fixing, write the cleaned content\n",
    "                cleaned_text = match_retry.group()\n",
    "                outfile.write(cleaned_text + \"\\n\")\n",
    "            else:\n",
    "                # Log unmatched lines for debugging (optional)\n",
    "                print(f\"Line {line_number}: No valid JSON-like content found even after fixing.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing close brackt ], If \"]\" is not in the line, code attempts to fix the line by replacing the last character with ]}\" (closing a JSON structure properly).\n",
    "\n",
    "\n",
    "input_file = \"/mntssd/mnt3/shanshanbai/nlpinearthobservation/synthetic_data/6000/temp.jsonl\"\n",
    "output_file = \"/mntssd/mnt3/shanshanbai/nlpinearthobservation/synthetic_data/6000/no_missing_brackets.jsonl\"\n",
    "\n",
    "# Process the file and fix lines without `]`\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as infile, open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for line_number, line in enumerate(infile, start=1):\n",
    "        if \"]\" not in line:\n",
    "            # Replace the last character with ])\" if ] is missing\n",
    "            fixed_line = line.strip()[:-1] + ']}\"' if line.strip() else line\n",
    "            outfile.write(fixed_line + \"\\n\")\n",
    "        else:\n",
    "            # Write the original line if it contains ]\n",
    "            outfile.write(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input and output file paths\n",
    "input_file = \"/mntssd/mnt3/shanshanbai/nlpinearthobservation/synthetic_data/6000/no_missing_brackets.jsonl\"\n",
    "output_file = \"/mntssd/mnt3/shanshanbai/nlpinearthobservation/synthetic_data/6000/between_brackets.jsonl\"\n",
    "\n",
    "# Process the file to extract content between the first [ and ] including the brackets\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as infile, open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for line_number, line in enumerate(infile, start=1):\n",
    "        # Match content between the first [ and the first ]\n",
    "        match = re.search(r\"\\[.*?\\]\", line)\n",
    "        if match:\n",
    "            extracted_content = match.group()  # Extract matched content including brackets\n",
    "            outfile.write(extracted_content + \"\\n\")  # Write the result with brackets\n",
    "        else:\n",
    "            print(f\"Line {line_number}: No content found between brackets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input and output file paths\n",
    "input_file = \"/mntssd/mnt3/shanshanbai/nlpinearthobservation/synthetic_data/6000/between_brackets.jsonl\"\n",
    "output_file = \"/mntssd/mnt3/shanshanbai/nlpinearthobservation/synthetic_data/6000/final.jsonl\"\n",
    "\n",
    "# Process the file to remove \\n and \\ characters\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as infile, open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for line in infile:\n",
    "        # Remove \\n and \\ characters\n",
    "        cleaned_line = line.replace(\"\\\\n\\\\\", \"\").replace(\"\\\\n\", \"\").replace(\"\\\\n \", \"\").replace(\"\\\\n  \", \"\").replace(\"\\\\n   \", \"\").replace(\"\\\\n    \", \"\").replace(\"\\\\\", \"\").replace(\"  \", \"\").replace(\"    \", \"\")\n",
    "        # Write the cleaned line to the output file\n",
    "        outfile.write(cleaned_line )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"/mntssd/mnt3/shanshanbai/nlpinearthobservation/synthetic_data/6000/final.jsonl\"\n",
    "output_file = \"/mntssd/mnt3/shanshanbai/nlpinearthobservation/synthetic_data/6000/final_.jsonl\"\n",
    "\n",
    "# Process the file\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as infile, open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for line_number, line in enumerate(infile, start=1):\n",
    "        # Remove leading/trailing whitespace\n",
    "        line = line.strip()\n",
    "\n",
    "        # Check if the second last character is \")\"\n",
    "        if len(line) > 1 and line[-2] == \")\":\n",
    "            # Remove the second last character\n",
    "            line = line[:-2] + line[-1]\n",
    "        \n",
    "        # Check if the second character is \"'\"\n",
    "        if len(line) > 1 and line[1] == \"'\":\n",
    "            # Remove the second last character\n",
    "            line = line[0] + '\"' + line[2:]\n",
    "        if len(line) > 1 and line[-2] == \"'\":\n",
    "            line = line[:-2] + '\"' + line[-1]\n",
    "        if len(line) > 1 and line[-2] == \",\":\n",
    "            # Remove the second last character\n",
    "            line = line[:-2] + line[-1]\n",
    "\n",
    "        # Write the fixed line to the output file\n",
    "        outfile.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the second last character is empty, remove that empty space\n",
    "\n",
    "input_file = \"/mntssd/mnt3/shanshanbai/nlpinearthobservation/synthetic_data/6000/final_.jsonl\"\n",
    "output_file = \"/mntssd/mnt3/shanshanbai/nlpinearthobservation/synthetic_data/6000/final__.jsonl\"\n",
    "\n",
    "# Process the file\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as infile, open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for line_number, line in enumerate(infile, start=1):\n",
    "        # Remove leading/trailing whitespace\n",
    "        line = line.strip()\n",
    "\n",
    "        # Check if the second last character is \")\"\n",
    "        if len(line) > 1 and line[-2] == \" \":\n",
    "            line = line[:-2] + line[-1]\n",
    "    \n",
    "\n",
    "        # Write the fixed line to the output file\n",
    "        outfile.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if the second last character is not \", then add \" in this place\n",
    "\n",
    "# input_file = \"/mntssd/mnt3/shanshanbai/nlpinearthobservation/synthetic_data/6000/final__.jsonl\"\n",
    "# output_file = \"/mntssd/mnt3/shanshanbai/nlpinearthobservation/synthetic_data/6000/final___.jsonl\"\n",
    "\n",
    "# # Process the file\n",
    "# with open(input_file, \"r\", encoding=\"utf-8\") as infile, open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "#     for line_number, line in enumerate(infile, start=1):\n",
    "#         line = line.strip()\n",
    "#         if len(line) > 1 and line[-2] != \",\":\n",
    "#             line = line[:-2] + '\"' + line[-1]\n",
    "    \n",
    "\n",
    "#         # Write the fixed line to the output file\n",
    "#         outfile.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input file path\n",
    "input_file = \"/mntssd/mnt3/shanshanbai/nlpinearthobservation/synthetic_data/6000/final___.jsonl\"\n",
    "\n",
    "# List to store lengths of each list\n",
    "list_lengths = []\n",
    "\n",
    "# Process the file and check the length of each list\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as infile:\n",
    "    for line_number, line in enumerate(infile, start=1):\n",
    "        try:\n",
    "            # Parse the line as a JSON list\n",
    "            tweet_list = json.loads(line.strip())\n",
    "            if isinstance(tweet_list, list):\n",
    "                # Get the length of the list and add it to the results\n",
    "                list_length = len(tweet_list)\n",
    "                list_lengths.append(list_length)\n",
    "            else:\n",
    "                print(f\"Line {line_number}: Not a valid list.\")\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Line {line_number}: JSON decode error - {e}\")\n",
    "\n",
    "# Output the results\n",
    "print(\"Lengths of each list:\", list_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input file path\n",
    "input_file = \"/mntssd/mnt3/shanshanbai/my_storage_from_qian/results/generated tweets/building_info_6000.jsonl\"\n",
    "\n",
    "# List to store lengths of \"tweet language distribution\"\n",
    "language_distribution_lengths = []\n",
    "\n",
    "# Process the file\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as infile:\n",
    "    for line_number, line in enumerate(infile, start=1):\n",
    "        try:\n",
    "            # Parse the line as a JSON object\n",
    "            data = json.loads(line.strip())\n",
    "            # Extract the \"tweet language distribution\" field and check its length\n",
    "            if \"tweet_language_distribution\" in data and isinstance(data[\"tweet_language_distribution\"], list):\n",
    "                list_length = len(data[\"tweet_language_distribution\"])\n",
    "                language_distribution_lengths.append(list_length)\n",
    "            else:\n",
    "                print(f\"Line {line_number}: 'tweet language distribution' field is missing or not a list.\")\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Line {line_number}: JSON decode error - {e}\")\n",
    "\n",
    "# Output the results\n",
    "# print(\"Lengths of 'tweet language distribution':\", language_distribution_lengths)\n",
    "\n",
    "\n",
    "# def count_mismatches(list1, list2):\n",
    "#     # Ensure both lists are the same length\n",
    "#     if len(list1) != len(list2):\n",
    "#         print(\"The lists have different lengths.\")\n",
    "#         return None\n",
    "\n",
    "#     # Count mismatches\n",
    "#     mismatches = sum(1 for a, b in zip(list1, list2) if a != b)\n",
    "#     return mismatches\n",
    "\n",
    "# # Count mismatches\n",
    "# mismatch_count = count_mismatches(list_lengths, language_distribution_lengths)\n",
    "\n",
    "# # Output the result\n",
    "\n",
    "# print(\"Number of mismatches:\", mismatch_count)\n",
    "\n",
    "# def create_mismatch_dicts(list1, list2):\n",
    "#     \"\"\"\n",
    "#     Finds the indexes of mismatched items between two lists.\n",
    "#     Returns a list of indexes where mismatches occur.\n",
    "#     \"\"\"\n",
    "#     min_length = min(len(list1), len(list2))\n",
    "#     dict_greater = {}  # For list1[i] > list2[i]\n",
    "#     dict_difference = {}  # For list1[i] < list2[i]\n",
    "\n",
    "#     # Compare elements at the same index\n",
    "#     for i in range(min_length):\n",
    "#         if list1[i] > list2[i]:\n",
    "#             dict_greater[i] = list2[i]\n",
    "#         elif list1[i] < list2[i]:\n",
    "#             dict_difference[i] = list2[i] - list1[i]\n",
    "\n",
    "#     return dict_greater, dict_difference\n",
    "\n",
    "# greater_dict, difference_dict = create_mismatch_dicts(list_lengths, language_distribution_lengths)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Input and output file paths\n",
    "# input_file = \"/mntssd/mnt3/shanshanbai/my_storage_from_qian/results/remove_bracket_.jsonl\"\n",
    "# output_file = \"/mntssd/mnt3/shanshanbai/my_storage_from_qian/results/remove_bracket_chopped.jsonl\"\n",
    "\n",
    "# # Example greater_dict with indices and corresponding values\n",
    "\n",
    "# # Process the file\n",
    "# with open(input_file, \"r\", encoding=\"utf-8\") as infile, open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "#     for line_number, line in enumerate(infile, start=1):\n",
    "#         try:\n",
    "#             # Parse the line as a JSON list\n",
    "#             row_list = json.loads(line.strip())\n",
    "#             if isinstance(row_list, list):\n",
    "#                 # Check if the current line number has an index in greater_dict\n",
    "#                 if line_number - 1 in greater_dict:  # Line numbers are 1-based, dict is 0-based\n",
    "#                     chop_index = greater_dict[line_number - 1]\n",
    "#                     # Chop the list up to chop_index\n",
    "#                     row_list = row_list[:chop_index]\n",
    "#                 # Write the modified list back to the file\n",
    "#                 outfile.write(json.dumps(row_list, ensure_ascii=False) + \"\\n\")\n",
    "#             else:\n",
    "#                 print(f\"Line {line_number}: Not a valid list.\")\n",
    "#         except json.JSONDecodeError as e:\n",
    "#             print(f\"Line {line_number}: JSON decode error - {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Input and output file paths\n",
    "# input_file = \"/mntssd/mnt3/shanshanbai/my_storage_from_qian/results/final.jsonl\"\n",
    "# output_file = \"/mntssd/mnt3/shanshanbai/my_storage_from_qian/results/final_final.jsonl\"\n",
    "\n",
    "# # Function to fix lines with a trailing `)`\n",
    "# def fix_trailing_parenthesis(line):\n",
    "#     line = line.strip()\n",
    "#     if line[-2] == ')':  # Check if the last second character is `)`\n",
    "#         line = line[:-2] + line[-1]  # Remove the last second character\n",
    "#     return line\n",
    "\n",
    "# # Process the file and fix lines if needed\n",
    "# with open(input_file, \"r\", encoding=\"utf-8\") as infile, open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "#     for line_number, line in enumerate(infile, start=1):\n",
    "#         try:\n",
    "#             # Parse the line as a JSON list\n",
    "#             tweet_list = json.loads(line.strip())\n",
    "#             if isinstance(tweet_list, list):\n",
    "#                 # Get the length of the list and write it back\n",
    "#                 # print(f\"Line {line_number}: Length = {len(tweet_list)}\")\n",
    "#                 outfile.write(json.dumps(tweet_list, ensure_ascii=False) + \"\\n\")\n",
    "#             else:\n",
    "#                 print(f\"Line {line_number}: Not a valid list.\")\n",
    "#         except json.JSONDecodeError as e:\n",
    "#             print(f\"Line {line_number}: JSON decode error - {e}\")\n",
    "#             # Attempt to fix the line by removing the trailing `)`\n",
    "#             fixed_line = fix_trailing_parenthesis(line)\n",
    "#             try:\n",
    "#                 # Retry parsing the fixed line\n",
    "#                 tweet_list = json.loads(fixed_line)\n",
    "#                 if isinstance(tweet_list, list):\n",
    "#                     print(f\"Line {line_number}: Fixed and Length = {len(tweet_list)}\")\n",
    "#                     outfile.write(json.dumps(tweet_list, ensure_ascii=False) + \"\\n\")\n",
    "#                 else:\n",
    "#                     print(f\"Line {line_number}: Fixed but not a valid list.\")\n",
    "#             except json.JSONDecodeError as retry_error:\n",
    "#                 print(f\"Line {line_number}: Still invalid after fix - {retry_error}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # verify if each line is a valid list\n",
    "\n",
    "# # Define the path to your JSONL file\n",
    "# input_file = \"/mntssd/mnt3/shanshanbai/nlpinearthobservation/synthetic_data/6000/final___.jsonl\"  # Replace with your actual file path\n",
    "\n",
    "# # Open and process the file line by line\n",
    "# with open(input_file, \"r\", encoding=\"utf-8\") as infile:\n",
    "#     for line_number, line in enumerate(infile, start=1):\n",
    "#         try:\n",
    "#             # Parse the JSON line\n",
    "#             parsed_data = json.loads(line.strip())\n",
    "\n",
    "#             # Check if the parsed data is a list\n",
    "#             if isinstance(parsed_data, list):\n",
    "#                 None\n",
    "#             else:\n",
    "#                 print(f\"❌ Line {line_number}: Not a list - Found {type(parsed_data)}\")\n",
    "\n",
    "#         except json.JSONDecodeError as e:\n",
    "#             print(f\"❌ Line {line_number}: Invalid JSON - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input file paths\n",
    "building_ids_file = \"/mntssd/mnt3/shanshanbai/my_storage_from_qian/results/generated tweets/building_info_6000.jsonl\"  # First file with building IDs\n",
    "tweets_file = \"/mntssd/mnt3/shanshanbai/nlpinearthobservation/synthetic_data/6000/final___.jsonl\"         # Second file with lists of tweets\n",
    "output_file = \"/mntssd/mnt3/shanshanbai/nlpinearthobservation/synthetic_data/6000/paper_dataset.jsonl\"             # Output file\n",
    "\n",
    "# List to store building IDs in their original sequence\n",
    "building_ids = []\n",
    "\n",
    "# Read and store building IDs in the order they appear\n",
    "with open(building_ids_file, \"r\", encoding=\"utf-8\") as infile:\n",
    "    for line in infile:\n",
    "        try:\n",
    "            data = json.loads(line.strip())\n",
    "            if \"building_id\" in data:\n",
    "                building_ids.append(data[\"building_id\"])\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing building IDs file: {e}\")\n",
    "            \n",
    "# Ensure building IDs and tweets are combined in the original sequence\n",
    "with open(tweets_file, \"r\", encoding=\"utf-8\") as infile, open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for building_id, line in zip(building_ids, infile):\n",
    "        try:\n",
    "            # Parse the line as a list of tweets\n",
    "            tweet_list = json.loads(line.strip())\n",
    "            if isinstance(tweet_list, list):\n",
    "                # Create a dictionary with the building id as the key\n",
    "                building_dict = {str(building_id): tweet_list}\n",
    "                # Write each dictionary as a JSON object on a new line\n",
    "                outfile.write(json.dumps(building_dict, ensure_ascii=False) + \"\\n\")\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing tweets file: {e}\")\n",
    "\n",
    "print(\"Building ID to Tweets Mapping saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge tweets and building files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# residential_tweet = \"/mntssd/mnt3/shanshanbai/my_storage_from_qian/results/final_residential.jsonl\"\n",
    "# commercial_tweet = \"/mntssd/mnt3/shanshanbai/my_storage_from_qian/results/final_commercial.jsonl\"\n",
    "# residential_building = \"/mntssd/mnt3/shanshanbai/my_storage_from_qian/results/residential_buildings_6558_.jsonl\"\n",
    "# commercial_building = \"/mntssd/mnt3/shanshanbai/my_storage_from_qian/results/commercial_buildings_7980_.jsonl\"\n",
    "\n",
    "# residential_commercial_tweet = \"/mntssd/mnt3/shanshanbai/my_storage_from_qian/results/residential_commercial_tweet\"\n",
    "# residential_commercial_building = \"/mntssd/mnt3/shanshanbai/my_storage_from_qian/results/residential_commercial_building\"\n",
    "\n",
    "tweets = \"/mntssd/mnt3/shanshanbai/nlpinearthobservation/synthetic_data/6000/paper_dataset.jsonl\"\n",
    "buildings = \"/mntssd/mnt3/shanshanbai/my_storage_from_qian/results/generated tweets/building_info_6000.jsonl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging complete! Data saved to 'merged_data.jsonl'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import ast  # To safely convert string representation of a list into an actual list\n",
    "\n",
    "# File paths (replace with actual file paths)\n",
    "metadata_file = \"/mntssd/mnt3/shanshanbai/my_storage_from_qian/results/generated tweets/building_info_6000.jsonl\"  # First file with building metadata\n",
    "tweets_file = \"/mntssd/mnt3/shanshanbai/nlpinearthobservation/synthetic_data/6000/paper_dataset.jsonl\"      # Second file with tweet data\n",
    "output_file = \"/mntssd/mnt3/shanshanbai/nlpinearthobservation/synthetic_data/6000/merged_data.jsonl\" # Output file\n",
    "\n",
    "# Step 1: Read the tweets file and store them in a dictionary\n",
    "tweets_dict = {}\n",
    "\n",
    "with open(tweets_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        try:\n",
    "            tweet_data = json.loads(line.strip())  # Parse JSON\n",
    "            for building_id, tweets in tweet_data.items():\n",
    "                tweets_dict[int(building_id)] = tweets  # Convert string ID to int\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing tweets file: {e}\")\n",
    "\n",
    "# Step 2: Read the metadata file, merge data, and write the final output\n",
    "with open(metadata_file, \"r\", encoding=\"utf-8\") as infile, open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for line in infile:\n",
    "        try:\n",
    "            # Parse the JSON object\n",
    "            building_data = json.loads(line.strip())\n",
    "\n",
    "            # Convert 'tweet_language_distribution' from string to actual list if necessary\n",
    "            if isinstance(building_data.get(\"tweet_language_distribution\"), str):\n",
    "                try:\n",
    "                    building_data[\"tweet_language_distribution\"] = ast.literal_eval(building_data[\"tweet_language_distribution\"])\n",
    "                except (ValueError, SyntaxError):\n",
    "                    print(f\"Warning: Could not parse tweet_language_distribution in line: {line.strip()}\")\n",
    "\n",
    "            # Retrieve tweets based on building_id\n",
    "            building_id = building_data[\"building_id\"]\n",
    "            building_data[\"tweets\"] = tweets_dict.get(building_id, [])  # Add tweets, default to empty list if not found\n",
    "\n",
    "            # Write the merged JSON object to the output file\n",
    "            json.dump(building_data, outfile, ensure_ascii=False)\n",
    "            outfile.write(\"\\n\")\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing metadata file: {e}\")\n",
    "\n",
    "print(\"Merging complete! Data saved to 'merged_data.jsonl'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
